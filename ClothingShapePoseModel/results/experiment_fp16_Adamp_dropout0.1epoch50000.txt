>>> Initializing clothing shape model...
>>> Star initialized.
>>> Get parameters list...
>>> Length of pose: 150
>>> Length of shape: 20
>>> Length of data: 3000
>>> 100/3000 loading...>>> 200/3000 loading...>>> 300/3000 loading...>>> 400/3000 loading...>>> 500/3000 loading...>>> 600/3000 loading...>>> 700/3000 loading...>>> 800/3000 loading...>>> 900/3000 loading...>>> 1000/3000 loading...>>> 1100/3000 loading...>>> 1200/3000 loading...>>> 1300/3000 loading...>>> 1400/3000 loading...>>> 1500/3000 loading...>>> 1600/3000 loading...>>> 1700/3000 loading...>>> 1800/3000 loading...>>> 1900/3000 loading...>>> 2000/3000 loading...>>> 2100/3000 loading...>>> 2200/3000 loading...>>> 2300/3000 loading...>>> 2400/3000 loading...>>> 2500/3000 loading...>>> 2600/3000 loading...>>> 2700/3000 loading...>>> 2800/3000 loading...>>> 2900/3000 loading...>>> 3000/3000 loading...
>>> MLP dimensions: [20752, 1024, 512, 256, 512, 5283].
>>> training MLP starts
training... 0100/50000: loss: 105.932training... 0200/50000: loss: 45.950training... 0300/50000: loss: 32.034training... 0400/50000: loss: 24.348training... 0500/50000: loss: 20.987training... 0600/50000: loss: 18.117training... 0700/50000: loss: 15.388training... 0800/50000: loss: 13.895training... 0900/50000: loss: 12.305training... 1000/50000: loss: 11.412training... 1100/50000: loss: 10.742training... 1200/50000: loss: 10.009training... 1300/50000: loss: 9.350training... 1400/50000: loss: 9.290training... 1500/50000: loss: 8.668training... 1600/50000: loss: 8.482training... 1700/50000: loss: 8.064training... 1800/50000: loss: 7.917training... 1900/50000: loss: 7.669training... 2000/50000: loss: 7.461training... 2100/50000: loss: 7.044training... 2200/50000: loss: 6.929training... 2300/50000: loss: 6.583training... 2400/50000: loss: 6.404training... 2500/50000: loss: 6.174training... 2600/50000: loss: 6.168training... 2700/50000: loss: 5.836training... 2800/50000: loss: 5.693training... 2900/50000: loss: 5.597training... 3000/50000: loss: 5.293training... 3100/50000: loss: 5.336training... 3200/50000: loss: 5.144training... 3300/50000: loss: 5.062training... 3400/50000: loss: 4.891training... 3500/50000: loss: 4.792training... 3600/50000: loss: 4.737training... 3700/50000: loss: 4.648training... 3800/50000: loss: 4.447training... 3900/50000: loss: 4.504training... 4000/50000: loss: 4.426training... 4100/50000: loss: 4.264training... 4200/50000: loss: 4.192training... 4300/50000: loss: 4.150training... 4400/50000: loss: 3.930training... 4500/50000: loss: 3.927training... 4600/50000: loss: 3.809training... 4700/50000: loss: 3.807training... 4800/50000: loss: 3.766training... 4900/50000: loss: 3.631training... 5000/50000: loss: 3.575training... 5100/50000: loss: 3.628training... 5200/50000: loss: 3.439training... 5300/50000: loss: 3.437training... 5400/50000: loss: 3.397training... 5500/50000: loss: 3.355training... 5600/50000: loss: 3.340training... 5700/50000: loss: 3.238training... 5800/50000: loss: 3.205training... 5900/50000: loss: 3.164training... 6000/50000: loss: 3.089training... 6100/50000: loss: 3.020training... 6200/50000: loss: 2.959training... 6300/50000: loss: 2.969training... 6400/50000: loss: 2.930training... 6500/50000: loss: 2.910training... 6600/50000: loss: 2.857training... 6700/50000: loss: 2.775training... 6800/50000: loss: 2.774training... 6900/50000: loss: 2.796training... 7000/50000: loss: 2.689training... 7100/50000: loss: 2.750training... 7200/50000: loss: 2.677training... 7300/50000: loss: 2.592training... 7400/50000: loss: 2.594training... 7500/50000: loss: 2.542training... 7600/50000: loss: 2.511training... 7700/50000: loss: 2.556training... 7800/50000: loss: 2.472training... 7900/50000: loss: 2.452training... 8000/50000: loss: 2.429training... 8100/50000: loss: 2.444training... 8200/50000: loss: 2.350training... 8300/50000: loss: 2.308training... 8400/50000: loss: 2.300training... 8500/50000: loss: 2.276training... 8600/50000: loss: 2.245training... 8700/50000: loss: 2.224training... 8800/50000: loss: 2.218training... 8900/50000: loss: 2.154training... 9000/50000: loss: 2.107training... 9100/50000: loss: 2.160training... 9200/50000: loss: 2.115training... 9300/50000: loss: 2.107training... 9400/50000: loss: 2.077training... 9500/50000: loss: 2.084training... 9600/50000: loss: 2.003training... 9700/50000: loss: 2.001training... 9800/50000: loss: 1.954training... 9900/50000: loss: 1.971training... 10000/50000: loss: 1.907training... 10100/50000: loss: 1.920training... 10200/50000: loss: 1.873training... 10300/50000: loss: 1.959training... 10400/50000: loss: 1.884training... 10500/50000: loss: 1.858training... 10600/50000: loss: 1.839training... 10700/50000: loss: 1.782training... 10800/50000: loss: 1.797training... 10900/50000: loss: 1.769training... 11000/50000: loss: 1.755training... 11100/50000: loss: 1.741training... 11200/50000: loss: 1.733training... 11300/50000: loss: 1.702training... 11400/50000: loss: 1.676training... 11500/50000: loss: 1.742training... 11600/50000: loss: 1.658training... 11700/50000: loss: 1.686training... 11800/50000: loss: 1.641training... 11900/50000: loss: 1.654training... 12000/50000: loss: 1.586training... 12100/50000: loss: 1.667training... 12200/50000: loss: 1.629training... 12300/50000: loss: 1.598training... 12400/50000: loss: 1.580training... 12500/50000: loss: 1.552training... 12600/50000: loss: 1.530training... 12700/50000: loss: 1.569training... 12800/50000: loss: 1.534training... 12900/50000: loss: 1.526training... 13000/50000: loss: 1.507training... 13100/50000: loss: 1.483training... 13200/50000: loss: 1.486training... 13300/50000: loss: 1.520training... 13400/50000: loss: 1.479training... 13500/50000: loss: 1.421training... 13600/50000: loss: 1.439training... 13700/50000: loss: 1.466training... 13800/50000: loss: 1.463training... 13900/50000: loss: 1.413training... 14000/50000: loss: 1.433training... 14100/50000: loss: 1.438training... 14200/50000: loss: 1.434training... 14300/50000: loss: 1.414training... 14400/50000: loss: 1.401training... 14500/50000: loss: 1.402training... 14600/50000: loss: 1.384training... 14700/50000: loss: 1.376training... 14800/50000: loss: 1.420training... 14900/50000: loss: 1.382training... 15000/50000: loss: 1.366training... 15100/50000: loss: 1.385training... 15200/50000: loss: 1.362training... 15300/50000: loss: 1.321training... 15400/50000: loss: 1.356training... 15500/50000: loss: 1.372training... 15600/50000: loss: 1.344training... 15700/50000: loss: 1.325training... 15800/50000: loss: 1.324training... 15900/50000: loss: 1.362training... 16000/50000: loss: 1.317training... 16100/50000: loss: 1.348training... 16200/50000: loss: 1.285training... 16300/50000: loss: 1.288training... 16400/50000: loss: 1.304training... 16500/50000: loss: 1.338training... 16600/50000: loss: 1.302training... 16700/50000: loss: 1.280training... 16800/50000: loss: 1.292training... 16900/50000: loss: 1.302training... 17000/50000: loss: 1.297training... 17100/50000: loss: 1.271training... 17200/50000: loss: 1.252training... 17300/50000: loss: 1.279training... 17400/50000: loss: 1.271training... 17500/50000: loss: 1.275training... 17600/50000: loss: 1.277training... 17700/50000: loss: 1.252training... 17800/50000: loss: 1.272training... 17900/50000: loss: 1.245training... 18000/50000: loss: 1.250training... 18100/50000: loss: 1.209training... 18200/50000: loss: 1.235training... 18300/50000: loss: 1.203training... 18400/50000: loss: 1.195training... 18500/50000: loss: 1.211training... 18600/50000: loss: 1.231training... 18700/50000: loss: 1.243training... 18800/50000: loss: 1.201training... 18900/50000: loss: 1.215training... 19000/50000: loss: 1.198training... 19100/50000: loss: 1.171training... 19200/50000: loss: 1.190training... 19300/50000: loss: 1.167training... 19400/50000: loss: 1.221training... 19500/50000: loss: 1.187training... 19600/50000: loss: 1.163training... 19700/50000: loss: 1.181training... 19800/50000: loss: 1.164training... 19900/50000: loss: 1.178training... 20000/50000: loss: 1.205training... 20100/50000: loss: 1.154training... 20200/50000: loss: 1.175training... 20300/50000: loss: 1.157training... 20400/50000: loss: 1.173training... 20500/50000: loss: 1.172training... 20600/50000: loss: 1.182training... 20700/50000: loss: 1.187training... 20800/50000: loss: 1.149training... 20900/50000: loss: 1.153training... 21000/50000: loss: 1.165training... 21100/50000: loss: 1.164training... 21200/50000: loss: 1.141training... 21300/50000: loss: 1.120training... 21400/50000: loss: 1.149training... 21500/50000: loss: 1.124training... 21600/50000: loss: 1.116training... 21700/50000: loss: 1.119training... 21800/50000: loss: 1.115training... 21900/50000: loss: 1.089training... 22000/50000: loss: 1.104training... 22100/50000: loss: 1.145training... 22200/50000: loss: 1.130training... 22300/50000: loss: 1.098training... 22400/50000: loss: 1.086training... 22500/50000: loss: 1.117training... 22600/50000: loss: 1.094training... 22700/50000: loss: 1.115training... 22800/50000: loss: 1.102training... 22900/50000: loss: 1.098training... 23000/50000: loss: 1.087training... 23100/50000: loss: 1.103training... 23200/50000: loss: 1.092training... 23300/50000: loss: 1.101training... 23400/50000: loss: 1.069training... 23500/50000: loss: 1.092training... 23600/50000: loss: 1.061training... 23700/50000: loss: 1.074training... 23800/50000: loss: 1.058training... 23900/50000: loss: 1.059training... 24000/50000: loss: 1.058training... 24100/50000: loss: 1.067training... 24200/50000: loss: 1.075training... 24300/50000: loss: 1.050training... 24400/50000: loss: 1.072training... 24500/50000: loss: 1.060training... 24600/50000: loss: 1.075training... 24700/50000: loss: 1.052training... 24800/50000: loss: 1.042training... 24900/50000: loss: 1.051training... 25000/50000: loss: 1.017training... 25100/50000: loss: 1.043training... 25200/50000: loss: 1.047training... 25300/50000: loss: 1.043training... 25400/50000: loss: 1.064training... 25500/50000: loss: 1.026training... 25600/50000: loss: 1.066training... 25700/50000: loss: 1.008training... 25800/50000: loss: 1.041training... 25900/50000: loss: 1.048training... 26000/50000: loss: 1.054training... 26100/50000: loss: 1.017training... 26200/50000: loss: 1.040training... 26300/50000: loss: 0.993training... 26400/50000: loss: 1.007training... 26500/50000: loss: 0.995training... 26600/50000: loss: 1.028training... 26700/50000: loss: 0.986training... 26800/50000: loss: 1.011training... 26900/50000: loss: 1.016training... 27000/50000: loss: 1.004training... 27100/50000: loss: 0.985training... 27200/50000: loss: 0.991training... 27300/50000: loss: 1.011training... 27400/50000: loss: 0.993training... 27500/50000: loss: 0.994training... 27600/50000: loss: 1.011training... 27700/50000: loss: 0.984training... 27800/50000: loss: 1.006training... 27900/50000: loss: 1.008training... 28000/50000: loss: 0.981training... 28100/50000: loss: 0.970training... 28200/50000: loss: 0.977training... 28300/50000: loss: 0.974training... 28400/50000: loss: 0.974training... 28500/50000: loss: 0.967training... 28600/50000: loss: 0.963training... 28700/50000: loss: 0.963training... 28800/50000: loss: 0.953training... 28900/50000: loss: 0.991training... 29000/50000: loss: 0.978training... 29100/50000: loss: 0.979training... 29200/50000: loss: 0.966training... 29300/50000: loss: 0.965training... 29400/50000: loss: 0.948training... 29500/50000: loss: 0.963training... 29600/50000: loss: 0.949training... 29700/50000: loss: 0.963training... 29800/50000: loss: 0.973training... 29900/50000: loss: 0.947training... 30000/50000: loss: 0.957training... 30100/50000: loss: 0.926training... 30200/50000: loss: 0.926training... 30300/50000: loss: 0.946training... 30400/50000: loss: 0.944training... 30500/50000: loss: 0.941training... 30600/50000: loss: 0.949training... 30700/50000: loss: 0.958training... 30800/50000: loss: 0.935training... 30900/50000: loss: 0.936training... 31000/50000: loss: 0.934training... 31100/50000: loss: 0.939training... 31200/50000: loss: 0.933training... 31300/50000: loss: 0.913training... 31400/50000: loss: 0.927training... 31500/50000: loss: 0.950training... 31600/50000: loss: 0.904training... 31700/50000: loss: 0.904training... 31800/50000: loss: 0.914training... 31900/50000: loss: 0.911training... 32000/50000: loss: 0.917training... 32100/50000: loss: 0.915training... 32200/50000: loss: 0.917training... 32300/50000: loss: 0.923training... 32400/50000: loss: 0.914training... 32500/50000: loss: 0.920training... 32600/50000: loss: 0.906training... 32700/50000: loss: 0.901training... 32800/50000: loss: 0.876training... 32900/50000: loss: 0.882training... 33000/50000: loss: 0.915training... 33100/50000: loss: 0.913training... 33200/50000: loss: 0.896training... 33300/50000: loss: 0.905training... 33400/50000: loss: 0.905training... 33500/50000: loss: 0.923training... 33600/50000: loss: 0.888training... 33700/50000: loss: 0.900training... 33800/50000: loss: 0.879training... 33900/50000: loss: 0.892training... 34000/50000: loss: 0.896training... 34100/50000: loss: 0.887training... 34200/50000: loss: 0.876training... 34300/50000: loss: 0.881training... 34400/50000: loss: 0.871training... 34500/50000: loss: 0.890training... 34600/50000: loss: 0.872training... 34700/50000: loss: 0.864training... 34800/50000: loss: 0.872training... 34900/50000: loss: 0.874training... 35000/50000: loss: 0.873training... 35100/50000: loss: 0.893training... 35200/50000: loss: 0.850training... 35300/50000: loss: 0.892training... 35400/50000: loss: 0.868training... 35500/50000: loss: 0.844training... 35600/50000: loss: 0.855training... 35700/50000: loss: 0.840training... 35800/50000: loss: 0.858training... 35900/50000: loss: 0.865training... 36000/50000: loss: 0.861training... 36100/50000: loss: 0.866training... 36200/50000: loss: 0.842training... 36300/50000: loss: 0.855training... 36400/50000: loss: 0.853training... 36500/50000: loss: 0.886training... 36600/50000: loss: 0.843training... 36700/50000: loss: 0.853training... 36800/50000: loss: 0.823training... 36900/50000: loss: 0.840training... 37000/50000: loss: 0.850training... 37100/50000: loss: 0.854training... 37200/50000: loss: 0.852training... 37300/50000: loss: 0.833training... 37400/50000: loss: 0.828training... 37500/50000: loss: 0.814training... 37600/50000: loss: 0.829training... 37700/50000: loss: 0.841training... 37800/50000: loss: 0.841training... 37900/50000: loss: 0.829training... 38000/50000: loss: 0.830training... 38100/50000: loss: 0.830training... 38200/50000: loss: 0.846training... 38300/50000: loss: 0.823training... 38400/50000: loss: 0.834training... 38500/50000: loss: 0.827training... 38600/50000: loss: 0.824training... 38700/50000: loss: 0.811training... 38800/50000: loss: 0.825training... 38900/50000: loss: 0.815training... 39000/50000: loss: 0.814training... 39100/50000: loss: 0.838training... 39200/50000: loss: 0.823training... 39300/50000: loss: 0.822training... 39400/50000: loss: 0.792training... 39500/50000: loss: 0.815training... 39600/50000: loss: 0.795training... 39700/50000: loss: 0.813training... 39800/50000: loss: 0.801training... 39900/50000: loss: 0.812training... 40000/50000: loss: 0.798training... 40100/50000: loss: 0.796training... 40200/50000: loss: 0.803training... 40300/50000: loss: 0.816training... 40400/50000: loss: 0.812training... 40500/50000: loss: 0.794training... 40600/50000: loss: 0.812training... 40700/50000: loss: 0.801training... 40800/50000: loss: 0.822training... 40900/50000: loss: 0.795training... 41000/50000: loss: 0.783training... 41100/50000: loss: 0.798training... 41200/50000: loss: 0.774training... 41300/50000: loss: 0.792training... 41400/50000: loss: 0.809training... 41500/50000: loss: 0.789training... 41600/50000: loss: 0.781training... 41700/50000: loss: 0.776training... 41800/50000: loss: 0.780training... 41900/50000: loss: 0.788training... 42000/50000: loss: 0.780training... 42100/50000: loss: 0.789training... 42200/50000: loss: 0.785training... 42300/50000: loss: 0.783training... 42400/50000: loss: 0.779training... 42500/50000: loss: 0.789training... 42600/50000: loss: 0.783training... 42700/50000: loss: 0.770training... 42800/50000: loss: 0.785training... 42900/50000: loss: 0.792training... 43000/50000: loss: 0.799training... 43100/50000: loss: 0.789training... 43200/50000: loss: 0.798training... 43300/50000: loss: 0.789training... 43400/50000: loss: 0.771training... 43500/50000: loss: 0.765training... 43600/50000: loss: 0.764training... 43700/50000: loss: 0.784training... 43800/50000: loss: 0.782training... 43900/50000: loss: 0.759training... 44000/50000: loss: 0.760training... 44100/50000: loss: 0.768training... 44200/50000: loss: 0.760training... 44300/50000: loss: 0.768training... 44400/50000: loss: 0.778training... 44500/50000: loss: 0.762training... 44600/50000: loss: 0.754training... 44700/50000: loss: 0.763training... 44800/50000: loss: 0.746training... 44900/50000: loss: 0.738training... 45000/50000: loss: 0.754training... 45100/50000: loss: 0.753training... 45200/50000: loss: 0.761training... 45300/50000: loss: 0.780training... 45400/50000: loss: 0.760training... 45500/50000: loss: 0.751training... 45600/50000: loss: 0.761training... 45700/50000: loss: 0.740training... 45800/50000: loss: 0.751training... 45900/50000: loss: 0.759training... 46000/50000: loss: 0.770training... 46100/50000: loss: 0.761training... 46200/50000: loss: 0.737training... 46300/50000: loss: 0.750training... 46400/50000: loss: 0.739training... 46500/50000: loss: 0.761training... 46600/50000: loss: 0.744training... 46700/50000: loss: 0.723training... 46800/50000: loss: 0.738training... 46900/50000: loss: 0.745training... 47000/50000: loss: 0.741training... 47100/50000: loss: 0.731training... 47200/50000: loss: 0.734training... 47300/50000: loss: 0.742training... 47400/50000: loss: 0.736training... 47500/50000: loss: 0.744training... 47600/50000: loss: 0.731training... 47700/50000: loss: 0.738training... 47800/50000: loss: 0.742training... 47900/50000: loss: 0.756training... 48000/50000: loss: 0.724training... 48100/50000: loss: 0.731training... 48200/50000: loss: 0.729training... 48300/50000: loss: 0.735training... 48400/50000: loss: 0.732training... 48500/50000: loss: 0.727training... 48600/50000: loss: 0.736training... 48700/50000: loss: 0.728training... 48800/50000: loss: 0.741training... 48900/50000: loss: 0.743training... 49000/50000: loss: 0.725training... 49100/50000: loss: 0.726training... 49200/50000: loss: 0.713training... 49300/50000: loss: 0.749training... 49400/50000: loss: 0.700training... 49500/50000: loss: 0.738training... 49600/50000: loss: 0.722training... 49700/50000: loss: 0.718training... 49800/50000: loss: 0.702training... 49900/50000: loss: 0.713training... 50000/50000: loss: 0.749
>>> It elapsed 35390.01 seconds for training
/home/cgna41/anaconda3/envs/hail/lib/python3.8/site-packages/torch/jit/_trace.py:958: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:
With rtol=1e-05 and atol=1e-05, found 5283 element(s) (out of 5283) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 0.11614207923412323 (0.03488682210445404 vs. 0.15102890133857727), which occurred at index (0, 2964).
  _check_trace(
>>> Star initialized.
>>> Draping...
>>> It takes 7.654 ms for draping
>>> OBJ file [./example/pose_0_10.obj] save completed.
>>> Draping...
>>> It takes 7.689 ms for draping
>>> OBJ file [./example/pose_1_10.obj] save completed.
>>> Draping...
>>> It takes 7.699 ms for draping
>>> OBJ file [./example/pose_2_10.obj] save completed.
>>> Draping...
>>> It takes 7.738 ms for draping
>>> OBJ file [./example/pose_3_10.obj] save completed.
>>> Draping...
>>> It takes 7.775 ms for draping
>>> OBJ file [./example/pose_4_10.obj] save completed.
>>> Draping...
>>> It takes 7.707 ms for draping
>>> OBJ file [./example/pose_5_10.obj] save completed.
>>> Draping...
>>> It takes 7.761 ms for draping
>>> OBJ file [./example/pose_6_10.obj] save completed.
>>> Draping...
>>> It takes 7.721 ms for draping
>>> OBJ file [./example/pose_7_10.obj] save completed.
>>> Draping...
>>> It takes 51.718 ms for draping
>>> OBJ file [./example/pose_8_10.obj] save completed.
>>> Draping...
>>> It takes 7.756 ms for draping
>>> OBJ file [./example/pose_9_10.obj] save completed.
>>> Draping...
>>> It takes 7.714 ms for draping
>>> OBJ file [./example/pose_10_10.obj] save completed.
