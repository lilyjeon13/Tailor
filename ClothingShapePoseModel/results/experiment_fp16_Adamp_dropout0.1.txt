>>> Initializing clothing shape model...
>>> Star initialized.
>>> Get parameters list...
>>> Length of pose: 150
>>> Length of shape: 20
>>> Length of data: 3000
>>> 100/3000 loading...>>> 200/3000 loading...>>> 300/3000 loading...>>> 400/3000 loading...>>> 500/3000 loading...>>> 600/3000 loading...>>> 700/3000 loading...>>> 800/3000 loading...>>> 900/3000 loading...>>> 1000/3000 loading...>>> 1100/3000 loading...>>> 1200/3000 loading...>>> 1300/3000 loading...>>> 1400/3000 loading...>>> 1500/3000 loading...>>> 1600/3000 loading...>>> 1700/3000 loading...>>> 1800/3000 loading...>>> 1900/3000 loading...>>> 2000/3000 loading...>>> 2100/3000 loading...>>> 2200/3000 loading...>>> 2300/3000 loading...>>> 2400/3000 loading...>>> 2500/3000 loading...>>> 2600/3000 loading...>>> 2700/3000 loading...>>> 2800/3000 loading...>>> 2900/3000 loading...>>> 3000/3000 loading...
>>> MLP dimensions: [20752, 2048, 2048, 2048, 2048, 2048, 5283].
>>> training MLP starts
training... 0100/100000: loss: 35.649training... 0200/100000: loss: 25.109training... 0300/100000: loss: 17.795training... 0400/100000: loss: 12.974training... 0500/100000: loss: 10.647training... 0600/100000: loss: 8.465training... 0700/100000: loss: 7.219training... 0800/100000: loss: 6.556training... 0900/100000: loss: 5.963training... 1000/100000: loss: 5.534training... 1100/100000: loss: 5.160training... 1200/100000: loss: 4.809training... 1300/100000: loss: 4.398training... 1400/100000: loss: 4.056training... 1500/100000: loss: 3.830training... 1600/100000: loss: 3.635training... 1700/100000: loss: 3.549training... 1800/100000: loss: 3.382training... 1900/100000: loss: 3.255training... 2000/100000: loss: 3.136training... 2100/100000: loss: 3.019training... 2200/100000: loss: 2.931training... 2300/100000: loss: 2.798training... 2400/100000: loss: 2.753training... 2500/100000: loss: 2.661training... 2600/100000: loss: 2.621training... 2700/100000: loss: 2.557training... 2800/100000: loss: 2.416training... 2900/100000: loss: 2.339training... 3000/100000: loss: 2.321training... 3100/100000: loss: 2.241training... 3200/100000: loss: 2.373training... 3300/100000: loss: 2.118training... 3400/100000: loss: 2.068training... 3500/100000: loss: 2.016training... 3600/100000: loss: 1.986training... 3700/100000: loss: 1.908training... 3800/100000: loss: 1.893training... 3900/100000: loss: 1.821training... 4000/100000: loss: 1.841training... 4100/100000: loss: 1.848training... 4200/100000: loss: 1.716training... 4300/100000: loss: 1.692training... 4400/100000: loss: 1.664training... 4500/100000: loss: 1.653training... 4600/100000: loss: 1.626training... 4700/100000: loss: 1.554training... 4800/100000: loss: 1.579training... 4900/100000: loss: 1.525training... 5000/100000: loss: 1.528training... 5100/100000: loss: 1.450training... 5200/100000: loss: 1.425training... 5300/100000: loss: 1.442training... 5400/100000: loss: 1.492training... 5500/100000: loss: 1.376training... 5600/100000: loss: 1.403training... 5700/100000: loss: 1.372training... 5800/100000: loss: 1.307training... 5900/100000: loss: 1.300training... 6000/100000: loss: 1.290training... 6100/100000: loss: 1.249training... 6200/100000: loss: 1.238training... 6300/100000: loss: 1.235training... 6400/100000: loss: 1.198training... 6500/100000: loss: 1.171training... 6600/100000: loss: 1.175training... 6700/100000: loss: 1.174training... 6800/100000: loss: 1.133training... 6900/100000: loss: 1.130training... 7000/100000: loss: 1.133training... 7100/100000: loss: 1.099training... 7200/100000: loss: 1.094training... 7300/100000: loss: 1.071training... 7400/100000: loss: 1.038training... 7500/100000: loss: 1.046training... 7600/100000: loss: 1.020training... 7700/100000: loss: 1.016training... 7800/100000: loss: 1.023training... 7900/100000: loss: 1.016training... 8000/100000: loss: 0.949training... 8100/100000: loss: 0.943training... 8200/100000: loss: 0.938training... 8300/100000: loss: 0.965training... 8400/100000: loss: 0.921training... 8500/100000: loss: 0.937training... 8600/100000: loss: 0.912training... 8700/100000: loss: 0.917training... 8800/100000: loss: 0.891training... 8900/100000: loss: 0.878training... 9000/100000: loss: 0.855training... 9100/100000: loss: 0.877training... 9200/100000: loss: 0.868training... 9300/100000: loss: 0.818training... 9400/100000: loss: 0.849training... 9500/100000: loss: 0.812training... 9600/100000: loss: 0.843training... 9700/100000: loss: 0.792training... 9800/100000: loss: 0.782training... 9900/100000: loss: 0.767training... 10000/100000: loss: 0.751training... 10100/100000: loss: 0.775training... 10200/100000: loss: 0.751training... 10300/100000: loss: 0.746training... 10400/100000: loss: 0.728training... 10500/100000: loss: 0.733training... 10600/100000: loss: 0.710training... 10700/100000: loss: 0.707training... 10800/100000: loss: 0.708training... 10900/100000: loss: 0.699training... 11000/100000: loss: 0.736training... 11100/100000: loss: 0.706training... 11200/100000: loss: 0.691training... 11300/100000: loss: 0.695training... 11400/100000: loss: 0.672training... 11500/100000: loss: 0.670training... 11600/100000: loss: 0.665training... 11700/100000: loss: 0.673training... 11800/100000: loss: 0.650training... 11900/100000: loss: 0.643training... 12000/100000: loss: 0.672training... 12100/100000: loss: 0.638training... 12200/100000: loss: 0.650training... 12300/100000: loss: 0.618training... 12400/100000: loss: 0.640training... 12500/100000: loss: 0.661training... 12600/100000: loss: 0.639training... 12700/100000: loss: 0.633training... 12800/100000: loss: 0.633training... 12900/100000: loss: 0.630training... 13000/100000: loss: 0.618training... 13100/100000: loss: 0.589training... 13200/100000: loss: 0.582training... 13300/100000: loss: 0.578training... 13400/100000: loss: 0.583training... 13500/100000: loss: 0.577training... 13600/100000: loss: 0.575training... 13700/100000: loss: 0.560training... 13800/100000: loss: 0.570training... 13900/100000: loss: 0.561training... 14000/100000: loss: 0.583training... 14100/100000: loss: 0.560training... 14200/100000: loss: 0.637training... 14300/100000: loss: 0.552training... 14400/100000: loss: 0.553training... 14500/100000: loss: 0.553training... 14600/100000: loss: 0.552training... 14700/100000: loss: 0.560training... 14800/100000: loss: 0.545training... 14900/100000: loss: 0.611training... 15000/100000: loss: 0.536training... 15100/100000: loss: 0.553training... 15200/100000: loss: 0.543training... 15300/100000: loss: 0.518training... 15400/100000: loss: 0.535training... 15500/100000: loss: 0.560training... 15600/100000: loss: 0.539training... 15700/100000: loss: 0.520training... 15800/100000: loss: 0.524training... 15900/100000: loss: 0.530training... 16000/100000: loss: 0.555training... 16100/100000: loss: 0.504training... 16200/100000: loss: 0.528training... 16300/100000: loss: 0.511training... 16400/100000: loss: 0.511training... 16500/100000: loss: 0.544training... 16600/100000: loss: 0.512training... 16700/100000: loss: 0.505training... 16800/100000: loss: 0.507training... 16900/100000: loss: 0.493training... 17000/100000: loss: 0.507training... 17100/100000: loss: 0.493training... 17200/100000: loss: 0.493training... 17300/100000: loss: 0.490training... 17400/100000: loss: 0.482training... 17500/100000: loss: 0.514training... 17600/100000: loss: 0.496training... 17700/100000: loss: 0.481training... 17800/100000: loss: 0.475training... 17900/100000: loss: 0.479training... 18000/100000: loss: 0.508training... 18100/100000: loss: 0.497training... 18200/100000: loss: 0.478training... 18300/100000: loss: 0.510training... 18400/100000: loss: 0.493training... 18500/100000: loss: 0.483training... 18600/100000: loss: 0.477training... 18700/100000: loss: 0.472training... 18800/100000: loss: 0.457training... 18900/100000: loss: 0.467training... 19000/100000: loss: 0.473training... 19100/100000: loss: 0.474training... 19200/100000: loss: 0.462training... 19300/100000: loss: 0.469training... 19400/100000: loss: 0.450training... 19500/100000: loss: 0.458training... 19600/100000: loss: 0.486training... 19700/100000: loss: 0.458training... 19800/100000: loss: 0.461training... 19900/100000: loss: 0.475training... 20000/100000: loss: 0.454training... 20100/100000: loss: 0.466training... 20200/100000: loss: 0.452training... 20300/100000: loss: 0.480training... 20400/100000: loss: 0.454training... 20500/100000: loss: 0.454training... 20600/100000: loss: 0.443training... 20700/100000: loss: 0.458training... 20800/100000: loss: 0.489training... 20900/100000: loss: 0.443training... 21000/100000: loss: 0.440training... 21100/100000: loss: 0.442training... 21200/100000: loss: 0.436training... 21300/100000: loss: 0.461training... 21400/100000: loss: 0.450training... 21500/100000: loss: 0.445training... 21600/100000: loss: 0.433training... 21700/100000: loss: 0.448training... 21800/100000: loss: 0.434training... 21900/100000: loss: 0.436training... 22000/100000: loss: 0.437training... 22100/100000: loss: 0.436training... 22200/100000: loss: 0.432training... 22300/100000: loss: 0.433training... 22400/100000: loss: 0.434training... 22500/100000: loss: 0.435training... 22600/100000: loss: 0.438training... 22700/100000: loss: 0.430training... 22800/100000: loss: 0.421training... 22900/100000: loss: 0.437training... 23000/100000: loss: 0.430training... 23100/100000: loss: 0.419training... 23200/100000: loss: 0.437training... 23300/100000: loss: 0.426training... 23400/100000: loss: 0.437training... 23500/100000: loss: 0.440training... 23600/100000: loss: 0.448training... 23700/100000: loss: 0.428training... 23800/100000: loss: 0.416training... 23900/100000: loss: 0.434training... 24000/100000: loss: 0.426training... 24100/100000: loss: 0.425training... 24200/100000: loss: 0.430training... 24300/100000: loss: 0.418training... 24400/100000: loss: 0.417training... 24500/100000: loss: 0.410training... 24600/100000: loss: 0.410training... 24700/100000: loss: 0.411training... 24800/100000: loss: 0.407training... 24900/100000: loss: 0.407training... 25000/100000: loss: 0.408training... 25100/100000: loss: 0.409training... 25200/100000: loss: 0.417training... 25300/100000: loss: 0.412training... 25400/100000: loss: 0.428training... 25500/100000: loss: 0.422training... 25600/100000: loss: 0.415training... 25700/100000: loss: 0.402training... 25800/100000: loss: 0.400training... 25900/100000: loss: 0.404training... 26000/100000: loss: 0.395training... 26100/100000: loss: 0.401training... 26200/100000: loss: 0.408training... 26300/100000: loss: 0.404training... 26400/100000: loss: 0.408training... 26500/100000: loss: 0.403training... 26600/100000: loss: 0.398training... 26700/100000: loss: 0.416training... 26800/100000: loss: 0.400training... 26900/100000: loss: 0.401training... 27000/100000: loss: 0.418training... 27100/100000: loss: 0.406training... 27200/100000: loss: 0.404training... 27300/100000: loss: 0.387training... 27400/100000: loss: 0.401training... 27500/100000: loss: 0.399training... 27600/100000: loss: 0.396